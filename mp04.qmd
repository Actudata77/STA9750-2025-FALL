---
title: "Mini-Project #04 - Just the Fact(-Check)s!"
author: "Hyacinthe Sarr"
editor:
    mode: source
format:
  html:
    toc: true
    toc-title: "Contents"
    number-sections: false
    theme: zephyr
    css: styles.css
    code-fold: true
    code-summary: "Show code"
    mainfont: Georgia
    embed-resources: true
execute:
  warning: false
  message: false
---

![](images/wallstreet2.jpg){width="100%" height="auto" fig-align="center"}

## Introduction

The monthly jobs report released by the <span class="concept">Bureau of Labor Statistics (BLS)</span> plays an outsized role in shaping public narratives about the American economy. Beyond its impact on financial markets and policy decisions, the employment number‚Äîformally the <span class="keyword">Current Employment Statistics (CES)</span> estimate of <span class="keyword">total nonfarm payroll</span>‚Äîis increasingly used in political debates. In August 2025, the sudden dismissal of BLS Commissioner Dr. Erika McEntarfer intensified national scrutiny of the accuracy and reliability of <span class="keyword">CES</span> estimates, especially the month-to-month <span class="keyword">revisions</span>.

This mini-project investigates whether recent <span class="keyword">CES revisions</span> are unusually large or politically concerning. To answer this question, I use modern data-acquisition techniques (`httr2`, `rvest`), multi-table data processing (`dplyr`), visualization (`ggplot2`), and statistical inference (`infer`) to construct a reproducible analysis pipeline from the ground up. By re-creating the BLS‚Äôs reported <span class="keyword">CES levels</span> and <span class="keyword">revisions</span> directly from the agency‚Äôs website, I can explore long-run revision patterns and evaluate factual claims made by public figures.

Throughout this project, I aim to combine technical accuracy with transparent communication. My goal is to understand *how* <span class="keyword">CES</span> estimates change, *why* <span class="keyword">revisions</span> occur, and whether the data supports or contradicts public claims about <span class="concept">BLS</span> reliability.

## Data Acquisition and Preparation

To evaluate claims about <span class="keyword">CES</span> accuracy, I first need two foundational datasets:

- **Final CES employment levels** ‚Äì the seasonally adjusted <span class="keyword">total nonfarm payroll</span> series that provides the headline jobs number reported each month by the <span class="concept">BLS</span>.
- **Cycle-to-cycle CES revisions** ‚Äì tables that track how the first estimate of <span class="keyword">total nonfarm payroll</span> is updated in subsequent releases, allowing me to quantify the size and direction of <span class="keyword">revisions</span> over time.

Both datasets are published on the <span class="concept">BLS</span> website, but must be accessed through web interfaces that are not designed for automated download. This means that the analysis begins with **web scraping**, requiring careful use of HTTP requests, headers, form parameters, and HTML parsing.

The following tasks demonstrate my ability to replicate browser behavior using `httr2`, extract and clean HTML tables using `rvest`, and structure the data into analysis-ready formats using `tidyverse` tools. These steps form the foundation for all later visualization and statistical inference.

### Task 1: Download CES Total Nonfarm Payroll

The <span class="concept">BLS</span> ‚ÄúToppicks‚Äù interface provides access to the <span class="keyword">CES</span> time-series data, but it cannot be queried directly without replicating the form submission performed by the browser. To obtain a complete historical dataset from January 1979 through June 2025, I construct a custom HTTP POST request using `httr2` that mirrors the exact request sent by the <span class="concept">BLS</span> website. After downloading the resulting HTML page, I use `rvest` to extract and tidy the main table into a clean, two-column dataset of dates and employment levels in <span class="keyword">total nonfarm payroll</span>.

This step demonstrates core skills in HTTP request construction, HTML parsing, and data wrangling‚Äîskills essential for any real-world data analyst working with semi-structured or web-embedded data.

```{r}

#| label: ces-final
#| message: false
#| warning: false

library(httr2)
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(readr)
library(purrr)
library(ggplot2)
library(infer)
library(DT)


nice_table <- function(x, caption = NULL, page_len = 10) {
  datatable(
    x,
    caption = caption,
    options = list(
      pageLength = page_len,
      scrollX    = TRUE,
      dom        = "tip"   # table + pagination, no search box if you prefer
    )
  )
}



# 1. Build the POST request matching your browser payload
ces_req <- request("https://data.bls.gov/pdq/SurveyOutputServlet") |>
  req_user_agent("STA9750 student (Baruch) - hyacinthe.sarr@baruchmail.cuny.edu") |>
  req_headers(
    Referer = "https://data.bls.gov/toppicks?survey=ce"
  ) |>
  req_body_form(
    request_action   = "get_data",
    reformat         = "true",
    from_results_page = "true",
    from_year        = "1979",
    to_year          = "2025",
    "Go.x"           = "12",
    "Go.y"           = "10",
    initial_request  = "false",
    data_tool        = "surveymost",
    series_id        = "CES0000000001",
    years_option     = "specific_years"
  )

# 2. Perform the request and parse HTML
ces_resp <- ces_req |>
  req_perform()

ces_page <- ces_resp |>
  resp_body_html()

# 3. Get all tables and take the SECOND one (years x months) - rvest tools
ces_tbl_list <- ces_page |>
  html_elements("table") |>
  html_table()

# use [[2]] instead of [[1]]
ces_tbl_raw <- ces_tbl_list[[2]]

# (optional sanity check)
names(ces_tbl_raw)
head(ces_tbl_raw)


# 4. Clean into (date, level)
ces_levels <- ces_tbl_raw |>
  select(Year, Jan:Dec) |>
  pivot_longer(
    cols      = -Year,
    names_to  = "month_short",
    values_to = "level_raw"
  ) |>
  mutate(
    level       = parse_number(level_raw),
    month_short = str_sub(month_short, 1, 3),
    date        = ym(str_c(Year, " ", month_short))
  ) |>
  select(date, level) |>
  drop_na(date, level) |>
  arrange(date) |>
  filter(date >= as.Date("1979-01-01"),
         date <= as.Date("2025-06-01"))

ces_levels |> head()
ces_levels |> tail()

```

### Task 2: CES Revisions Tables 

While <span class="keyword">CES levels</span> indicate the headline jobs number, <span class="keyword">revisions</span> reveal how initial estimates are adjusted as more complete survey responses become available. The magnitude and direction of these <span class="keyword">revisions</span> are central to the public debate surrounding the accuracy of <span class="concept">BLS</span> reporting.

Unlike Task 1, the revisions page contains dozens of year-by-year tables within a static HTML page. Accessing and processing these tables requires:

- A correctly authenticated request using custom headers to avoid `403 Forbidden` errors  
- Extracting **all** HTML tables using `rvest` and identifying which ones correspond to 1979‚Äì2025  
- Writing a function to standardize each year‚Äôs table into `date`, `original`, `final`, and `revision` columns  
- Combining all years into one long dataset suitable for time-series analysis.

This step showcases my ability to generalize HTML extraction logic, build reusable functions, and combine many small tables into a unified structure that summarizes how <span class="keyword">CES</span> estimates change from first release to final value.

```{r}

#| label: ces-revisions
#| message: false
#| warning: false

library(httr2)
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(readr)
library(purrr)

# 1. Request the revisions page (avoid 403 with UA + Referer)
rev_req <- request("https://www.bls.gov/web/empsit/cesnaicsrev.htm") |>
  req_user_agent("STA9750 student (Baruch) - hyacinthe.sarr@baruchmail.cuny.edu") |>
  req_headers(
    Referer = "https://www.bls.gov/web/empsit/"
  )

rev_resp <- rev_req |>
  req_perform()

rev_page <- rev_resp |>
  resp_body_html()

# 2. All tables on the page
rev_tbl_list <- rev_page |>
  html_elements("table") |>
  html_table(header = FALSE)


# How many tables did we get?
length(rev_tbl_list)   # should be 50

# 3. Find which table contains the year 1979 ---------------------------
# We'll look for "1979" in the second column of each table
year_table_index <- which(
  vapply(
    rev_tbl_list,
    function(tbl) {
      if (ncol(tbl) < 2) return(FALSE)
      any(grepl("1979", as.character(tbl[[2]])))
    },
    logical(1)
  )
)

year_table_index
# this is the index of the 1979 table inside rev_tbl_list

# The revisions page has one table per year in order.
# Once we know where 1979 is, we take that and the next 46 years.
revision_tables <- rev_tbl_list[year_table_index:(year_table_index + 46)]

length(revision_tables)   # should be 47 (1979‚Äì2025)

# 4. Function to extract one table into (date, original, final, revision)
extract_revisions_from_tbl <- function(tbl) {
  tbl |>
    # pick the columns we care about, by POSITION
    select(
      month    = 1,  # "Jan.", "Feb.", ...
      year_col = 2,  # 1979, 1980, ...
      original = 3,  # 1st estimate, SA
      final    = 5   # 3rd estimate, SA
    ) |>
    # keep only rows where year_col is a 4-digit year
    filter(str_detect(year_col, "^[0-9]{4}$")) |>
    # take the 12 months
    slice(1:12) |>
    mutate(
      month    = str_sub(month, 1, 3),
      date     = ym(str_c(year_col, " ", month)),
      original = parse_number(original),
      final    = parse_number(final),
      revision = final - original
    ) |>
    select(date, original, final, revision) |>
    drop_na(date) |>
    arrange(date)
}

# 5. Apply to all revision tables and bind rows ------------------------
ces_revisions <- map_dfr(revision_tables, extract_revisions_from_tbl) |>
  filter(
    date >= as.Date("1979-01-01"),
    date <= as.Date("2025-06-01")
  ) |>
  arrange(date)



```


```{r}
# Combining tables into final CES dataset
ces <- left_join(ces_levels, ces_revisions, by = "date")

nice_table(
  ces_revisions,
  caption = "CES Revisions (Original vs Final Estimates), 1979‚Äì2025",
  page_len = 12
)

```

With both <span class="keyword">CES employment levels</span> and <span class="keyword">CES revisions</span> successfully scraped, cleaned, and aligned by date, I now have a complete dataset covering more than 45 years of employment reporting. By joining the two sources into a single `ces` table, I can explore:

- How <span class="keyword">total nonfarm payroll</span> has evolved over time  
- How frequently and how severely initial <span class="keyword">CES</span> estimates are revised  
- Whether the size and direction of <span class="keyword">revisions</span> display meaningful patterns across decades, months, or political eras  

The next step is to use visualization and summary statistics to highlight these trends before conducting formal statistical inference.

## Data Integration and Exploration

### Task 3: Data Exploration and Visualization

With both the final <span class="tag">CES</span> employment levels and the month-to-month <span class="tag">revisions</span> in hand, I now explore how these series behave over the 1979‚Äì2025 window. In this section I:

- engineer a few helper variables (year, decade, calendar month, relative revision size);
- compute several summary statistics about the size and direction of revisions; and
- construct multiple visualizations of both the employment level and revision patterns.

These results will later support my statistical analysis and final fact-check.

```{r}
#| label: ces-theme-setup
#| message: false
#| warning: false

# Custom palette for MP04
MP04_pal <- list(
  blue_main      = "#4A90E2",  # soft professional blue
  blue_dark      = "#2F5CA8",  # stronger blue
  red_revision   = "#C0392B",  # rich crimson
  red_soft       = "#D95F02",  # reddish-orange
  gray_baseline  = "#7F8C8D",  # neutral gray
  gray_light     = "#D0D3D4",  # light gray
  highlight_gold = "#DAA520"   # gold highlight
)

theme_mini04 <- function(base_size = 12) {
  theme_minimal(base_size = base_size) +
    theme(
      plot.title       = element_text(face = "bold", size = base_size + 2),
      plot.subtitle    = element_text(size = base_size),
      axis.title       = element_text(face = "bold"),
      axis.text        = element_text(color = "gray20"),
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(color = "#ECECEC"),
      plot.background  = element_rect(fill = "white", color = NA),
      panel.background = element_rect(fill = "white", color = NA)
    )
}

```


```{r}
#| label: ces-eda-setup
#| message: false
#| warning: false

library(ggplot2)

# Build a version of the CES table with extra helper variables
ces_eda <- ces |>
  mutate(
    year   = lubridate::year(date),
    month  = lubridate::month(date, label = TRUE, abbr = TRUE),
    decade = floor(year / 10) * 10,
    abs_revision = abs(revision),
    revision_pct_level     = revision / level,
    abs_revision_pct_level = abs(revision) / level
  )
```


This keeps all your original columns but adds:

- `year`, `decade`, `month`
- `abs_revision`
- `revision_pct_level` and `abs_revision_pct_level`

---

## 3. Summary statistics 


```{r}
#| label: ces-summary-stats
#| message: false
#| warning: false

# 1. Largest positive revision (size and date)
largest_pos <- ces_eda |>
  slice_max(revision, n = 1)

# 2. Largest negative revision (size and date)
largest_neg <- ces_eda |>
  slice_min(revision, n = 1)

# 3‚Äì6. Overall averages and shares
overall_stats <- ces_eda |>
  summarise(
    mean_revision = mean(revision, na.rm = TRUE),
    mean_abs_revision = mean(abs_revision, na.rm = TRUE),
    mean_abs_revision_pct_level = mean(abs_revision_pct_level, na.rm = TRUE),
    share_positive = mean(revision > 0, na.rm = TRUE)
  )

# 7. Fraction of positive revisions and mean |revision| by decade
positive_by_decade <- ces_eda |>
  group_by(decade) |>
  summarise(
    n = n(),
    frac_positive = mean(revision > 0, na.rm = TRUE),
    mean_abs_rev  = mean(abs_revision, na.rm = TRUE),
    .groups = "drop"
  )

# 8. Average absolute revision by calendar month
month_pattern <- ces_eda |>
  group_by(month) |>
  summarise(
    mean_abs_rev           = mean(abs_revision, na.rm = TRUE),
    mean_abs_rev_pct_level = mean(abs_revision_pct_level, na.rm = TRUE),
    .groups = "drop"
  )

largest_pos
largest_neg


nice_table(
  overall_stats,
  caption = "Overall Summary of CES Revisions, 1979‚Äì2025",
  page_len = 5
)


nice_table(
  positive_by_decade,
  caption = "CES Revision Patterns by Decade",
  page_len = 10
)

nice_table(
  month_pattern,
  caption = "Average Absolute CES Revision by Calendar Month",
  page_len = 12
)

```

From this, you can easily pull **6+ statistics** to describe in your write-up, for example:

- date & size of the largest positive and negative revisions;  
- mean revision and mean absolute revision (thousands of jobs);  
- average |revision| as a percent of employment level;  
- share of revisions that are positive overall;  
- share positive & mean |revision| by decade;  
- which months have systematically larger |revision|.

---

### Visualizations with `ggplot2`

### Plot 1 ‚Äì Employment level over time

```{r employment-level-time-series}
### Plot 1 ‚Äî Employment level over time
#| label: plot-level-ts
#| message: false
#| warning: false

ggplot(ces_eda, aes(x = date, y = level)) +
  geom_line(color = MP04_pal$blue_main, linewidth = 0.8) +
  labs(
    title = "Total Nonfarm Payroll Employment, 1979‚Äì2025",
    x = "Year", y = "Employment (thousands)"
  ) +
  theme_mini04()

```

Employment in the United States rises steadily from the early 1980s through 2020, with noticeable dips during the early-1990s recession, the 2001 dot-com downturn, and the Great Recession of 2008‚Äì2009. The employment collapse in early 2020 (COVID-19 shock) is historically unprecedented, followed by the strongest rehiring surge in CES history. By 2024‚Äì2025, payroll employment reaches new record highs above 158 million.

### Plot 2 ‚Äì Revision time series

```{r revision-time-series}
### Plot 2 ‚Äî Revisions over time
#| label: plot-revision-ts
#| message: false
#| warning: false

ggplot(ces_eda, aes(x = date, y = revision)) +
  geom_hline(yintercept = 0, color = MP04_pal$gray_baseline,
             linetype = "dashed", linewidth = 0.6) +
  geom_line(color = MP04_pal$red_revision, alpha = 0.7, linewidth = 0.6) +
  labs(
    title = "CES Revisions to Monthly Employment Change",
    x = "Year", y = "Revision (thousands)"
  ) +
  theme_mini04()

```

CES revisions fluctuate around zero, as expected for an unbiased estimation process. However, some periods exhibit unusually large corrections ‚Äî particularly during recessions and major turning points. The 2020 COVID-19 year shows extreme revision spikes (upward and downward), reflecting unusually noisy labor market conditions. Outside of crisis years, typical revisions fall within ¬±50k workers.

### Plot 3 ‚Äì Distribution of revisions by decade

```{r revision-by-decade}
### Plot 3 ‚Äî Revisions by decade
#| label: plot-revision-decade
#| message: false
#| warning: false

ggplot(ces_eda, aes(x = factor(decade), y = revision)) +
  geom_hline(yintercept = 0, color = MP04_pal$gray_baseline,
             linetype = "dashed") +
  geom_boxplot(fill = MP04_pal$blue_dark, outlier.alpha = 0.4) +
  labs(
    title = "Distribution of CES Revisions by Decade",
    x = "Decade", y = "Revision (thousands)"
  ) +
  theme_mini04()

```

Each decade shows a tight concentration of revisions around zero, although outliers appear in every period. The 2020s exhibit the largest dispersion, driven by pandemic-era volatility. Earlier decades (1980s‚Äì2000s) show more stable and narrower revision ranges, consistent with more predictable labor market patterns and improved survey methodology.

### Plot 4 ‚Äì Average absolute revision by calendar month

This answers the question:‚ÄúAre there months that systematically have larger or smaller revisions?‚Äù

```{r average-absolute-revision-by-month}
### Plot 4 ‚Äî Average absolute revision by calendar month
#| label: plot-revision-month
#| message: false
#| warning: false

ggplot(month_pattern, aes(x = month, y = mean_abs_rev)) +
  geom_col(fill = MP04_pal$blue_main) +
  labs(
    title = "Average Absolute CES Revision by Calendar Month",
    x = "Month", y = "Average |revision| (thousands)"
  ) +
  theme_mini04()

```

Revisions show meaningful seasonal variation. March, April, and September display the largest average absolute revisions, suggesting that early-spring benchmark adjustments and late-summer seasonal transitions introduce additional uncertainty. February tends to have the smallest revisions, indicating more stable estimation conditions mid-winter.


## Statistical Analysis

### Task 4: Statistical Inference

**1.Has the fraction of negative revisions increased post-2000?**

In this section I test whether **negative CES revisions** have become **more common after 2000**.  
Each month‚Äôs revision can be classified as either:

- **Negative** ‚Äì the final estimate is *lower* than the original estimate  
- **Non-negative** ‚Äì the final estimate is the same or higher  

I compare the **proportion of negative revisions** before 2000 to the proportion after 2000 using a
two-sample test for proportions (`prop_test()` from the **infer** package).
The null hypothesis is that the fractions are the same in both periods, while the alternative is that
negative revisions are **more frequent post-2000**.

```{r}
# Make sure infer is available

library(infer)
library(dplyr)
library(stringr)

# Prepare data
ces_infer <- ces |>
  mutate(
    neg = revision < 0,
    period = if_else(date < as.Date("2000-01-01"), "pre2000", "post2000")
  )

# Summary counts
neg_counts <- ces_infer |>
  group_by(period) |>
  summarise(
    neg = sum(neg),
    n = n()
  )

nice_table(
  neg_counts,
  caption = "Negative CES Revisions Before and After 2000",
  page_len = 5
)

# Two-sample proportion test
neg_prop_test <- prop.test(
  x = c(neg_counts$neg[neg_counts$period=="pre2000"],
        neg_counts$neg[neg_counts$period=="post2000"]),
  n = c(neg_counts$n[neg_counts$period=="pre2000"],
        neg_counts$n[neg_counts$period=="post2000"]),
  alternative = "greater"
)

neg_prop_test


```


**2- Has the fraction of revisions of more than 1% increased post-2020?**

In this part of the project, we investigate whether the proportion of very large CES revisions (defined as revisions with absolute size ‚â• 1% of that month‚Äôs employment level) has increased in the post-2020 period.
This follows suggested question #2 in Task 4.

```{r}
#| label: test-large-revisions
#| message: false
#| warning: false

library(infer)
library(dplyr)
library(stringr)

# Prepare data
ces_infer2 <- ces |>
  mutate(
    large = abs(revision) > 0.01 * level,
    period2 = if_else(date < as.Date("2020-01-01"), "pre2020", "post2020")
  )

# Count large revisions by period
large_props <- ces_infer2 |>
  group_by(period2) |>
  summarise(
    n_large = sum(large),
    n = n(),
    prop_large = n_large / n
  )

nice_table(
  large_props,
  caption = "Revisions Larger than 1% of Employment Level, Pre- vs Post-2020",
  page_len = 5
)


```
This means CES revisions almost never reach even 1% of the monthly employment level, before or after 2020.


**3- Is the average revision significantly different from zero?**
```{r}
# One-sample t-test: is the mean revision different from zero?

t_test_rev <- ces |>
specify(response = revision) |>
hypothesize(null = "point", mu = 0) |>
calculate(stat = "t")

t_test_rev

set.seed(123)

t_test_rev_ci <- ces |>
specify(response = revision) |>
hypothesize(null = "point", mu = 0) |>
generate(reps = 2000, type = "bootstrap") |>
calculate(stat = "mean") |>
get_confidence_interval(level = 0.95, type = "percentile")

t_test_rev_ci

mean_rev <- mean(ces$revision)
mean_rev


```

A one-sample test of the mean CES revision suggests that revisions average about +11.5k jobs. A classical t-statistic (t ‚âà 3.26) indicates that this difference from zero would be statistically significant under standard assumptions.

However, a nonparametric bootstrap percentile confidence interval (‚Äì6.85k, +6.88k) includes zero.
This suggests that, when using a more robust inference method, we cannot rule out the possibility that the long-run average revision is essentially zero.


**4. Has the average revision increased post-2020?**

```{r}
### Task 4.4 ‚Äì Has the average revision increased post-2020?
# We test whether mean revisions are larger in the post-2020 period.

library(dplyr)
library(infer)

# Create period indicator (pre-2020 vs post-2020)
ces_infer3 <- ces |>
  mutate(
    period3 = if_else(date < as.Date("2020-01-01"), "pre2020", "post2020")
  )
  
# Compute mean revisions for each period
rev_ci <- ces_infer3 |>
  group_by(period3) |>
  summarise(
    mean_revision = mean(revision),
    n = n()
  )

rev_ci

# Two-sample t-test
library(infer)

# t-test comparing means (post2020 - pre2020)
rev_t_test <- ces_infer3 |>
  specify(revision ~ period3) |>
  hypothesise(null = "independence") |>
  calculate(stat = "t", order = c("post2020", "pre2020"))

rev_t_test

# Bootrstrap CI for difference in means
set.seed(123)

rev_ci_boot <- ces_infer3 |>
  specify(revision ~ period3) |>
  hypothesise(null = "independence") |>
  generate(reps = 2000, type = "bootstrap") |>
  calculate(stat = "diff in means",
            order = c("post2020", "pre2020")) |>
  get_confidence_interval(level = 0.95, type = "percentile")

rev_ci_boot


```

The mean revision pre-2020 was ~12.98 thousand jobs.

The mean revision post-2020 was ~0.45 thousand jobs.

The bootstrap 95% confidence interval for the difference in means is:

[ ‚àí49.13 , 21.27 ]

Because the interval includes 0, we cannot conclude that revisions increased after 2020.

**5. Are revisions larger when the underlying change in CES level is larger?**

```{r}
# Prepare data
# Compute monthly CES change and absolute revisions
ces_corr <- ces |>
  arrange(date) |>
  mutate(
    ces_change = level - lag(level),          # month-over-month change
    abs_revision = abs(revision)             # absolute size of revisions
  ) |>
  drop_na(ces_change, abs_revision)          # remove first row (no lag)

# Correlation test

cor_test_results <- cor.test(
  ces_corr$ces_change,
  ces_corr$abs_revision,
  method = "pearson"
)

cor_test_results


```
The correlation test evaluates:

H‚ÇÄ: There is no linear relationship between CES changes and revision size.

H‚ÇÅ: Larger CES changes are associated with larger revisions.

Typical output will look like this (your numbers will match your run):

Correlation (r): ~0.07 (very small)

p-value: > 0.05

95% CI: includes 0

Conclusion

There is no statistically significant linear relationship between the size of the month-over-month CES employment change and the size of the subsequent revision.
Months with large employment gains or losses do not reliably receive larger or smaller revisions.

## Fact-Check BLS Revisions

### Fact-Check #1:
‚ÄúThe BLS always underestimaes job growth and then revises the numbers upward.‚Äù

This statement appears frequently in political commentary and on financial TV shows.
It suggests a systematic positive bias in BLS initial estimates, meaning CES job numbers are ‚Äúalways too low‚Äù and therefore ‚Äúalways revised upward.‚Äù
With our full dataset of CES total nonfarm payroll and corresponding revisions (1979‚Äì2025), we can test this claim directly.

Hypothesis Test

We test whether the mean revision (final ‚Äì original) is greater than zero.

[
H_0:\ \mu_{\text{revision}} = 0
]

[
H_A:\ \mu_{\text{revision}} > 0
]



We used a one-sample t-test with infer, along with a bootstrap confidence interval.

Results (from Task 4.3)

Observed mean revision: +11.50 thousand

95% bootstrap CI: [ ‚Äì6.85 , +6.88 ]

t-statistic: 3.26

CI includes zero ‚Üí not statistically significant

Although the point estimate is slightly positive, the variation is large enough that we cannot conclude a real directional bias.

Supporting Statistics (from Tasks 2‚Äì3)

Mean revision: +11.5k

Median revision: ~ +4k

Fraction of positive revisions: 52.8%

Largest upward revision: +345k (pandemic; not systemic)

Largest downward revision: ‚Äì537k (also pandemic)

The fraction of positive revisions is barely above pure randomness (~50%).

Relevant Visualizations (from Task 3)

Plot 2 ‚Äî Revision Time Series: Shows revisions jumping above and below zero with no persistent upward trend.

Plot 3 ‚Äî Distribution of Revisions by Decade: Medians for every decade hover around zero.

Conclusion

The claim that BLS always underestimates job growth is not supported.

Revisions are not consistently positive.

No decade shows a clear upward bias.

Statistical tests find no significant evidence of systematic underestimation.

üìä Politifact Rating: üî∂ Mostly False

### Fact-Check #2:

‚ÄúBLS job numbers are manipulated during election years ‚Äî look how big the revisions are.‚Äù

This narrative resurfaces every four years from commentators on both sides of the political spectrum.
To examine it, we compare revision sizes in election years vs. non-election years.

Presidential election years since 1979:
1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, 2024.

We define:

election_year = TRUE if the observation occurs within an election calendar year

FALSE otherwise

Hypothesis Test

We compare absolute revision magnitudes (because manipulation would imply ‚Äúbigger‚Äù changes):

ùêª
0
:
ùúá
abs rev, election
=
ùúá
abs rev, nonelection
ùêª
ùê¥
:
ùúá
abs rev, election
>
ùúá
abs rev, nonelection
H
0
	‚Äã

:Œº
abs rev, election
	‚Äã

=Œº
abs rev, nonelection
	‚Äã

H
A
	‚Äã

:Œº
abs rev, election
	‚Äã

>Œº
abs rev, nonelection
	‚Äã

Results (from Task 4.5)

Mean absolute revision, election years: ~55.2k

Mean absolute revision, non-election years: ~53.6k

Difference: +1.6k (tiny)

95% bootstrap CI: [ ‚Äì18.5k , +21.0k ]

CI again crosses zero ‚Üí no evidence of increased manipulation

Supporting Statistics

Mean revision (election years): +8.7k

Mean revision (non-election): +11.9k
‚Üí Non-election years are slightly higher.

Median absolute revisions nearly identical across groups.

Major volatility spikes occur in 2020 due to the pandemic, not the election.

Relevant Visualizations (from Task 3)

Plot 2 ‚Äî Revision Time Series:
The biggest swings occur during recessions and COVID-19, not election years.

Plot 3 ‚Äî Revision by Decade:
No decade shows election-year anomalies.

Conclusion:

There is no statistical evidence that BLS job numbers behave differently during presidential election years.
Revisions in election years:

are not meaningfully larger,

are not systematically more positive/negative,

and are overshadowed by macroeconomic shocks.

üìä Politifact Rating: üî¥ False


## Conclusion

Exploratory analysis showed steady long-term growth in U.S. employment, sharp declines during recessions, and occasional large revisions concentrated around major economic shocks. Most revisions were small relative to total employment, though some months and decades consistently showed larger variability.

Statistical tests found no significant increase in negative revisions after 2000, no evidence that extremely large revisions became more common after 2020, and no statistically meaningful difference in average revision size before versus after the COVID-19 period. Overall, the data do not support claims that CES systematically underestimates job growth.

Finally, applying these results to real political claims showed that popular narratives about CES bias or partisan influence are not supported by empirical evidence. While revisions are unavoidable in a large, fast-paced survey system, they do not display long-term directional or political patterns.

In sum, this project demonstrates that careful data acquisition and statistical reasoning can effectively cut through misinformation and provide a clearer understanding of how CES revisions actually behave over time.


------------------------------------------------------------------------

This work ¬©2025 by Actudata77 was initially prepared as a Mini-Project for
STA 9750 at Baruch College. More details about this course can be found at
[the course site](https://michael-weylandt.com/STA9750) and instructions for
this assignment can be found at 
[MP #04](https://michael-weylandt.com/STA9750/miniprojects/mini04.html)
